# Text Mining

> Is machine learning the best solution to text mining?\What if graph theory beats it in both time and space complexity?

The answer is obvious, definitely not. The graph theory is definitely an underestimated data structure, compared to all that hype of machine learning. First, let's talk about what this project is and later you would realize why graph structure works better. Graph theory turns out to beat supervised learning in both time and space complexity. 

This project is designed to be integrated into my scraping script called <a href=https://github.com/je-suis-tm/web-scraping/blob/master/MENA%20Newsfeed.py>MENA newsfeed</a>. Initially, the script would scrape news titles from different mainstream websites (so-called fake news, lol) including BBC, CNN, Reuters, Al Jazeera etc. All these scraped news titles, links and preview images would be concatnated into one HTML email and automatically send it to my inbox every morning. After a couple of days, I realized that many websites were actually reporting the same story but in different titles and preview images. It was totally a waste of energy and time to read these similar contents over and over again. And not every piece of information was worth my time to read. Some stories such as 'British Iranian woman got put in jail' was not exactly business related (I'm sorry, it is still a great story though). Couldn't I find a way to create a filter to extract the key information?

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/email.PNG)

The first thing came to my mind, of course to anyone, was machine learning. We could build up a classifier to determine which one is key information. A simple Naive Bayes Multivariate Event Model would do the trick. The methodology is basically based on the assumption that when certain key words such as 'oil','crude','south pars' and 'LNG' come together in a title, the title is more likely to be oil business related. It turned out that Bernoulli Naive Bayes Classifier worked pretty well for key information screening. The accuracy was always capped at 70 to 80 percent. It could be improved by building up a better stopword list. The downside of this is the time complexity. It is always the issue of supervised learning especially if it is generative learning. The default sklearn package needs to calculate the conditional probability of each word in the vocabulary list repeatedly when making a forecast. In terms of the execution speed, the whole process could be boosted with a little computer science technique called memoization. I did write a <a href=https://github.com/je-suis-tm/machine-learning/blob/master/optimized%20naive%20bayes%20with%20memoization.py>self implementation of Naive Bayes</a> that its time complexity was greatly optimized. Still, that didn't solve the problem of similar contents with different titles. You may argue that I wasn't working hard enough. We could build up a two dimensional dataframe with the length of n*(n-1)/2 assuming n is how many titles we have scraped. Yes, we could, and we convert the dataframe into a multi-dimensional vocabulary matrix. Support Vector Machine or Random Forrest or any other classifiers could help us to remove the similar contents and extract the key information. Well, our time and space complexity issue still exists. We still have to manually classify everything and save a training dataset on a local drive.

Is that the only trick up my sleeves? Nope, what about graph theory? We could connect news titles from different sources together on the criteria of how many words they have in common. As usual for natural language processing, we always need to keep a stopword list to exclude some stop words. In our case, some country names such as 'saudi arabia' or 'iran' or 'tehran' should be included in the stopword list. Unfortunately, the node names of a graph structure cannot be the exact news titles. We do not want to have a node name consists of more than fifteen words. In this case, the index of the news title in a concatnated dataframe would be presented as the node name. The edge between two nodes would be established if and only if two news titles share some words in common. The number of common words would be denoted as the weight of the edge. But there is another problem, how can we connect word 'walking' with word 'walked'? Well, in that case, we also need to include a stemming process. Currently NLTK is the most popular tool for text mining. Even though the famous Porter stemmer is not very effective, as it's a rule based stemmer instead of dictionary based. English, is a terrible language for its messy vocabulary rules. Keeping a dictionary based stemmer would require a lot of space. Hence, we would have to cope with the imperfect Porter stemmer.

Let's visualize the graph network and see what is really happening. In the following figure, the color of the edge reflects the weight of the edge. The more words two news titles share in common, the warmer the edge color is. Some titles may not have any word in common with the rest of the titles. These titles do not appear in this graph structure. Later on, we could add these rebellious titles back to our output. For the moment, we are only concerned with the nodes in the graph ADT.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/original.png)

Let's look at the assumption of this approach and find out why the graph structure is going to work. Assuming a piece of story is a breaking news, it is so important that every mainstream media would cover it. Maybe different media websites cover the story from a different perspective but at least they produce similar content just in relatively different titles. The script called MENA newsfeed scrapes a lot of websites. Hence, there must be some similar contents with different titles from different sources. These titles should be connected to each other as the similar content always should have at least one key information in common. This assumption may sound very confusing. Looking at this example, we have the following titles, 'Airstrike on Yemen school bus is apparent war crime' from CNN, 'UN accuses Saudi coalition of war crimes in Yemen' from Financial Times, 'Mistakes admitted in Yemen bus attack' from BBC. These titles are connected by the common words 'school bus' and 'war crime' ('yemen' is a stop word so not included). The content 'yemen bus attack is a war crime' exists in every media website with a different title. We only want to see this content once in our email instead of three times from different sources. The title of the content with the most common words with others is denoted as our target. 

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/edge.png)

In a graph structure, our targets could be presented as key nodes of strongly connected components. In the following visualization, black squares highlight strongly connected components and red circles highlight key nodes which either have the most edges in a given strongly connected component or connect to other strongly connected components.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/initial%20target.png)

Is there any known traversal algorithm that can return a list of our target nodes? Not to my knowledge (If you do, please feel free to comment). Nevertheless, our selection criteria is not complex and we could always implement our own version of traversal algorithm. The first thing that came to my mind was <a href=https://github.com/je-suis-tm/graph-theory/blob/master/BFS%20DFS%20on%20DCG.ipynb>Breadth First Search</a>. Think of target nodes as parent nodes, all we need to examine is that parent nodes are the nodes with the most edges in any given strong connected component. This BFS, I call it Alternative BFS, would start at each node in the graph structure. Each starting node is defined as a parent node. It would go one layer deeper to the child nodes. Each traversal from the parent node to all child nodes returns a tree structure. The tree structure would be denoted as a strongly connected component. The Alter BFS is designed to select a node with most edges in a given strongly connected component and append the node to an output list. When two nodes have the same number of edges, the algorithm would look at the total sum of weights of each node's edges. The node with highest sum of weights would go to the output list. If the sum of weights cannot select a winner, it is then on a first come first served basis. Whichever node the algorithm travels first would be selected. In the following graph, red nodes are the nodes selected by Alter BFS. Unfortunately, the visualization layout of networkx is random unless we specify the fixed position for each node (which implies a lot of work). I tried my very best to keep all nodes in consistent positions throughout these figures.

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs%20demo.png)
![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs.png)
![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/bfs%20result.png)
![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/small%20target.png)

Despite the fact that this is an elegant traversal algorithm, we could tell that some nodes are still redundant. Even if we use set() function to remove duplicates from our output list, we could still end up with a lot of false positive. For instance, we have node alpha, beta and gamma. Node alpha connects to node beta and others. Node alpha has 4 edges and node beta only has 2 edges. Node beta has two edges, which connects to node alpha and node gamma.And node gamma only has one edge connected to node beta. So when Alter BFS runs on node alpha and node beta, we only keep node alpha in check. When Alter BFS runs on node gamma, as node gamma is not connected to node alpha and node beta has more edges than node gamma, we would also append node beta in the output list. But node beta and node alpha are connected and apparently node alpha is the target node we defined. Thus, we need another round of iteration to remove one node from any two connected nodes in the output list. 


![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/temp%20result.png)
![alt text](https://raw.githubusercontent.com/je-suis-tm/graph-theory/master/Text%20Mining%20project/preview/remove%20children%20demo.png)

The methodology is pretty much the same as Alter BFS. First, we check if two nodes are connected. If so, we compare the number of edges for each node and remove those which have smaller number of edges. If both nodes have the same number of edges, the total sum of weights of each node would be the selection criteria. Again, if the total sum of weights cannot tell a winner, it is always on a first come first served basis.

Voila! This is the result of our graph theory based text mining! Unlike supervised learning, the algorithm doesn't require an extra large memory for training dataset. It beats machine learning in both time and space complexity. 

![alt text](https://github.com/je-suis-tm/graph-theory/blob/master/Text%20Mining%20project/preview/final%20result.png)

Even though the graph traversal algorithm has given us the key information, there is still work to do. For those nodes not included in our graph structure, they may be some niche information which is exclusive to one particular website. It is best to append them to the output list. 

Finally, is this approach flawless? Nope, no algorithm is perfect. As much as I love about the algorithm I developed, Alter BFS could still miss some information from the graph structure. Consider we have two nodes, 'Nazanin Zaghari-Ratcliffe back in Tehran prison' and 'Some 400 prisoners escape prison in Tripoli chaos'. They are connected by the common word 'prison' but they appear to be completely different contents. I suppose there is always a tradeoff among different algorithms. Is there any room for improvement? Yes and always. We could implement a heuristic technique as in <a href=https://github.com/je-suis-tm/graph-theory/blob/master/a_star%20maze.ipynb>Graph Theory - A*</a>. In our alternative BFS traversal, our condition would be the number of edges plus the total sum of the weights instead of the number of edges alone. 

In terms of time, space and accuracy, apparently graph structure traversal is a better approach for this scenario rather than machine learning. It answers the question, machine learning is not the golden solution to everything. Graph theory also has the potential to solve sophisticated text mining problems as well.
